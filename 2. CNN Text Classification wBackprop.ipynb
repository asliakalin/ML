{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CNN_Forward_Backward_Passes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asliakalin/ML/blob/master/CNN_Text_Classification_wBackprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6sGKifyWQ6S",
        "colab_type": "text"
      },
      "source": [
        "# Homework 2: Convolutional Neural Networks for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw6pcjjzWdqI",
        "colab_type": "text"
      },
      "source": [
        "In this homework, you will be implementing the _forward pass_ and _backpropagation_ for a convolutional neural network with sparse inputs for text classification. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hiu0tLm9Wnah",
        "colab_type": "text"
      },
      "source": [
        "## The setup\n",
        "Let's define parameters for the Convolutional Neural Network. You do not need to modify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "284_yXERWZFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# window size for the CNN\n",
        "width = 2\n",
        "\n",
        "# number of filters\n",
        "F = 100\n",
        "\n",
        "# learning rate\n",
        "alpha = 1e-1\n",
        "\n",
        "# vocabsize: size of the total vocabulary\n",
        "vocabsize = 10000\n",
        "\n",
        "# vocab: the vocabulary dictionary with the word as key and its index as value\n",
        "# the input will be transformed into respective positional indices using the vocab dictionary\n",
        "# as the input for the forward and backward algorithm\n",
        "# e.g. if vocab = {'a': 0, 'simple': 1, 'sentence': 2} and the training data is\n",
        "# \"a simple simple sentence a\",\n",
        "# the input to the forward and backward algorithm will be [0,1,1,2,0]\n",
        "vocab = {}\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# U and V are weight vectors of the hidden layer\n",
        "# U: a matrix of weights of all inputs for the first\n",
        "# hidden layer for all F filters in the\n",
        "# where each filter has the size of vocabsize by width (window size)\n",
        "# U[i, j, k] represents the weight of filter u_j for word with vocab[word] = i when the word is at the position k of the sliding window\n",
        "\n",
        "# e.g. for the example, \"a simple simple sentence a\",\n",
        "# if the window size is 4 and we are looking at the first sliding window\n",
        "# of the 9th filter, the weight for the last \"sentence\" will be U[2, 8, 3]\n",
        "\n",
        "# U[index of the word in vocab, index of the filter, position of the word in that sliding window]\n",
        "U = np.random.normal(loc=0, scale=0.01, size=(vocabsize, F, width))\n",
        "\n",
        "# V: the the weight vector of the F filter outputs (after max pooling)\n",
        "# that will produce the output, i.e. o = sigmoid(V*h)\n",
        "V = np.random.normal(loc=0, scale=0.01, size=(F))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t0ZPDoiFzp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w_i = np.zeros(vocabsize)\n",
        "w_i[2] = 1\n",
        "\n",
        "w_ii = np.zeros(vocabsize)\n",
        "w_ii[3] = 1\n",
        "\n",
        "val = np.tanh(U[2, 0, 0]+ U[3, 0, 1])\n",
        "#print(U)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw0w-FwHXfH_",
        "colab_type": "text"
      },
      "source": [
        "Let's define some utility functions that may be useful. You don't need to modify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHNxHBX7WrYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    helper function that computes the sigmoid function\n",
        "    \"\"\"\n",
        "    return 1. / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "def read_vocab(filename):\n",
        "    \"\"\"\n",
        "    helper function that builds up the vocab dictionary for input transformation\n",
        "    \"\"\"\n",
        "    file = open(filename)\n",
        "    for line in file:\n",
        "        cols = line.rstrip().split(\"\\t\")\n",
        "        word = cols[0]\n",
        "        idd = int(cols[1])\n",
        "        vocab[word] = idd\n",
        "    file.close()\n",
        "\n",
        "\n",
        "def read_data(filename):\n",
        "    \"\"\"\n",
        "    :param filename: the name of the file\n",
        "    :return: list of tuple ([word index list], label)\n",
        "    as input for the forward and backward function\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    file = open(filename)\n",
        "    for line in file:\n",
        "        cols = line.rstrip().split(\"\\t\")\n",
        "        label = int(cols[0])\n",
        "        words = cols[1].split(\" \")\n",
        "        w_int = []\n",
        "        for w in words:\n",
        "            # skip the unknown words\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "        data.append((w_int, label))\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "\n",
        "def train():\n",
        "    \"\"\"\n",
        "    main caller function that reads in the names of the files\n",
        "    and train the CNN to classify movie reviews\n",
        "    \"\"\"\n",
        "    vocabFile = \"vocab.txt\"\n",
        "    trainingFile = \"movie_reviews.train\"\n",
        "    testFile = \"movie_reviews.dev\"\n",
        "\n",
        "    read_vocab(vocabFile)\n",
        "    training_data = read_data(trainingFile)\n",
        "    test_data = read_data(testFile)\n",
        "\n",
        "    for i in range(50):\n",
        "        #print(i)\n",
        "        # confusion matrix showing the accuracy of the algorithm\n",
        "        confusion_training = np.zeros((2, 2))\n",
        "        confusion_validation = np.zeros((2, 2))\n",
        "\n",
        "        for (data, label) in training_data:\n",
        "            # back propagation to update weights for both U and V\n",
        "            backward(data, label)\n",
        "\n",
        "            # calculate forward and evaluate\n",
        "            prob = forward(data)[\"prob\"]\n",
        "            pred = 1 if prob > .5 else 0\n",
        "            confusion_training[pred, label] += 1\n",
        "\n",
        "        for (data, label) in test_data:\n",
        "            # calculate forward and evaluate\n",
        "            prob = forward(data)[\"prob\"]\n",
        "            pred = 1 if prob > .5 else 0\n",
        "            confusion_validation[pred, label] += 1\n",
        "\n",
        "        print(\"Epoch: {} \\tDev accuracy: {:.3f}\"\n",
        "            .format(\n",
        "            i,\n",
        "            np.sum(np.diag(confusion_validation)) / np.sum(confusion_validation)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev6O33rf25Ko",
        "colab_type": "text"
      },
      "source": [
        "And finally, we'll download the data. We'll be doing sentiment analysis on a dataset of movie reviews, so we'll need 3 files - a vocabulary file, a file with a training set of movie reviews, and a development set containing different reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0Q7-x7-2_d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_2/vocab.txt \n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_2/movie_reviews.dev\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_2/movie_reviews.train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5I1QY3NXnkX",
        "colab_type": "text"
      },
      "source": [
        "## 1. Forward\n",
        "\n",
        "Given the parameters and definition of the CNN model (ยง2 of HW), complete the Forward Function to calculate _o_ (the probability of the positive class) for an input text. You may not import any additional libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bpdg6uXxKCFW",
        "colab_type": "code",
        "outputId": "3b630476-b2e0-432c-e934-d49a4510ed19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "w_i = np.zeros(5)\n",
        "w_i[1] = 1\n",
        "\n",
        "w_ii = np.zeros(5)\n",
        "w_ii[3] = 1\n",
        "\n",
        "x_i = np.stack((w_i, w_ii), axis=1)\n",
        "print(x_i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0.]\n",
            " [1. 0.]\n",
            " [0. 0.]\n",
            " [0. 1.]\n",
            " [0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR7JOqL9XjZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(word_indices):\n",
        "    \"\"\"\n",
        "    :param word_indices: a list of word indices, i.e. idx = vocab[word]\n",
        "    :return: a result dictionary containing 3 items -\n",
        "    result['prob']: output of the CNN algorithm.\n",
        "    result['h']: the hidden layer output after max pooling, h = [h1, ..., hF]\n",
        "    result['hid']: argmax of F filters, e.g. j of x_j\n",
        "    e.g. for the ith filter u_i, tanh(word[hid[i], hid[i] + width]*u_i) = max(h_i)\n",
        "    \"\"\"\n",
        "\n",
        "    h = np.zeros(F, dtype=float)\n",
        "    hid = np.zeros(F, dtype=int)\n",
        "    prob = 0.0\n",
        "\n",
        "    # step 1. compute h and hid\n",
        "    # loop through the input data of word indices and\n",
        "    # keep track of the max filtered value h_i over all windows using filter i, and its position index x_j\n",
        "    # h_i = max(tanh(weighted sum of all words in a given window over all windows for u_i\n",
        "    \n",
        "    for j in range(F): # j represents filter\n",
        "      p_j = np.zeros(len(word_indices)-width+1) \n",
        "      # array of vals using same filter across different windows\n",
        "      # same size as number of window sequences\n",
        "\n",
        "      for i in range(len(word_indices)-width+1): # i represents the window starting position\n",
        "        p_j[i] = np.tanh(U[word_indices[i], j,0] + U[word_indices[i+1], j, 1]) \n",
        "        # adds resulting value of i'th window sequence to index i of p_j\n",
        "\n",
        "      h[j] = max(p_j)\n",
        "      hid[j] = np.argmax(p_j) \n",
        "\n",
        "    # step 2. compute probability\n",
        "    # once h and hid are computed, compute the probabiliy by sigmoid(h^TV)\n",
        "    prob = sigmoid(np.dot(h, V))\n",
        "    \n",
        "    # step 3. return result\n",
        "    return {\"prob\": prob, \"h\": h, \"hid\": hid}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E74JyTjkXvqK",
        "colab_type": "text"
      },
      "source": [
        "## 2. Backward\n",
        "\n",
        "Using the gradient update equations for V (ยง3 in HW) and U (ยง3.1), implement the updates for U and V in the backward function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFyGR2BWZ0YF",
        "colab_type": "code",
        "outputId": "b384fd25-aea1-406f-d024-4032811d6fdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "u_i = np.stack([U[5,7,0], U[10,7,1]])\n",
        "u_i.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT8hRNr0XsVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(word_indices, true_label):\n",
        "    \"\"\"\n",
        "    :param word_indices: a list of word indices, i.e. idx = vocab[word]\n",
        "    :param true_label: true label (0, 1) of the movie reviews\n",
        "    :return: None\n",
        "    update weight matrix/vector U and V based on the loss function\n",
        "    \"\"\"\n",
        "    global U, V\n",
        "    pred = forward(word_indices)\n",
        "    prob = pred[\"prob\"]\n",
        "    h = pred[\"h\"]\n",
        "    hid = pred[\"hid\"]\n",
        "\n",
        "    # update U and V here\n",
        "    # loss_function = y * log(o) + (1 - y) * log(1 - o) \n",
        "    #               = true_label * log(prob) + (1 - true_label) * log(1 - prob)\n",
        "    # to update V: V_new = V_current + d(loss_function)/d(V)*alpha\n",
        "    # to update U: U_new = U_current + d(loss_function)/d(U)*alpha\n",
        "    # U[i, j, k] represents the weight of filter u_j for word with vocab[word] = i when the word is at the position k of the sliding window\n",
        "    N = len(word_indices)\n",
        "    window = width\n",
        "    \n",
        "    # L wrt h:\n",
        "    dLdh = (true_label - prob)*V\n",
        "\n",
        "    # h wrp p, L wrt p:\n",
        "    dLdp = np.zeros((N-window+1, F))\n",
        "    dhdp = np.zeros((F, N-window+1))\n",
        "    for j in range(F):\n",
        "      argmax_i = hid[j]\n",
        "      dhdp[j][argmax_i] = 1\n",
        "      dLdp[argmax_i][j] = 1*dLdh[j]\n",
        "\n",
        "    #p wrt u\n",
        "    dpdu = np.zeros((vocabsize, F, window))\n",
        "    for j in range(F):\n",
        "      argmax_i = hid[j]\n",
        "      x_i = np.zeros((vocabsize, window))\n",
        "      x_i[word_indices[argmax_i]][0] = 1\n",
        "      x_i[word_indices[argmax_i+1]][1] = 1\n",
        "      u_i = np.stack([U[word_indices[argmax_i],j,0], U[word_indices[argmax_i+1],j,1]])\n",
        "      dpdu[word_indices[argmax_i],j] = np.matmul((1 - (np.tanh(np.matmul(x_i,u_i))**2)), x_i)\n",
        "      \n",
        "    # update U and V\n",
        "    for i in range(len(word_indices)-window): \n",
        "      for f in range(F):\n",
        "        U[word_indices[i],f,0] += alpha*(dLdp[i][f]*dpdu[word_indices[i],f,0])\n",
        "        U[word_indices[i+1],f,1] += alpha*(dLdp[i+1][f]*dpdu[word_indices[i+1],f,1]) \n",
        "\n",
        "    V = V + alpha * (true_label - prob) * h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atJ7itphH0U_",
        "colab_type": "text"
      },
      "source": [
        "Once you have implemented both the forward and backward functions, your can test out your implementations by training the model. To do so, run the `train` function in the cell below. If your implementations are correct, you should see the accuracy improve as the model trains (You will be graded based on the correctness of the implementations, not on this accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S8d23Uf5K9Q",
        "colab_type": "code",
        "outputId": "01b4db2c-4f72-422e-e3ff-e9c27beed67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tDev accuracy: 0.520\n",
            "Epoch: 1 \tDev accuracy: 0.560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT6ieh9h0X9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
