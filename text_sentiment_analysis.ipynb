{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asli Akalin - HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asliakalin/ML/blob/master/text_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bymVftHXImRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT7xD4k2QiCH",
        "colab_type": "code",
        "outputId": "8e32ee7a-1577-47e1-adbd-eb8eea273cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "#This code gets the train/dev/test files from github and imports them into Colab\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/test.txt.zip\n",
        "!unzip test.txt.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-11 09:14:40--  https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1427184 (1.4M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>]   1.36M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-02-11 09:14:40 (10.5 MB/s) - ‘train.txt’ saved [1427184/1427184]\n",
            "\n",
            "--2020-02-11 09:14:41--  https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1474560 (1.4M) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>]   1.41M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-02-11 09:14:42 (22.9 MB/s) - ‘dev.txt’ saved [1474560/1474560]\n",
            "\n",
            "--2020-02-11 09:14:43--  https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/test.txt.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13100860 (12M) [application/zip]\n",
            "Saving to: ‘test.txt.zip’\n",
            "\n",
            "test.txt.zip        100%[===================>]  12.49M  36.3MB/s    in 0.3s    \n",
            "\n",
            "2020-02-11 09:14:44 (36.3 MB/s) - ‘test.txt.zip’ saved [13100860/13100860]\n",
            "\n",
            "Archive:  test.txt.zip\n",
            "  inflating: test.txt                \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._test.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYRARhMtI7AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the dumb features the model starts with.\n",
        "######################################################################\n",
        "def dumb_featurize(text):\n",
        "\tfeats = {}\n",
        "\twords = text.split(\" \")\n",
        "\tfor word in words:\n",
        "\t\tif word == \"love\" or word == \"like\" or word == \"best\":\n",
        "\t\t\tfeats[\"contains_positive_word\"] = 1\n",
        "\t\tif word == \"hate\" or word == \"dislike\" or word == \"worst\" or word == \"awful\":\n",
        "\t\t\tfeats[\"contains_negative_word\"] = 1\n",
        "\treturn feats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGiM8qQiJOBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the sentiment classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class SentimentClassifier:\n",
        "\n",
        "\tdef __init__(self, feature_method):\n",
        "\t\tself.feature_vocab = {}\n",
        "\t\tself.feature_method = feature_method\n",
        "\n",
        "\n",
        "\t# Read data from file\n",
        "\tdef load_data(self, filename):\n",
        "\t\tdata = []\n",
        "\t\twith open(filename, encoding=\"utf8\") as file:\n",
        "\t\t\tfor line in file:\n",
        "\t\t\t\tcols = line.split(\"\\t\")\n",
        "\t\t\t\tlabel = cols[0]\n",
        "\t\t\t\ttext = cols[1].rstrip()\n",
        "\n",
        "\t\t\t\tdata.append((label, text))\n",
        "\t\treturn data\n",
        "\n",
        "\t# Featurize entire dataset\n",
        "\tdef featurize(self, data):\n",
        "\t\tfeaturized_data = []\n",
        "\t\tfor label, text in data:\n",
        "\t\t\tfeats = self.feature_method(text)\n",
        "\t\t\tfeaturized_data.append((label, feats))\n",
        "\t\treturn featurized_data\n",
        "\n",
        "\t# Read dataset and returned featurized representation as sparse matrix + label array\n",
        "\tdef process(self, dataFile, training = False):\n",
        "\t\tdata = self.load_data(dataFile)\n",
        "\t\tdata = self.featurize(data)\n",
        "\n",
        "\t\tif training:\t\t\t\n",
        "\t\t\tfid = 0\n",
        "\t\t\tfeature_doc_count = Counter()\n",
        "\t\t\tfor label, feats in data:\n",
        "\t\t\t\tfor feat in feats:\n",
        "\t\t\t\t\tfeature_doc_count[feat]+= 1\n",
        "\n",
        "\t\t\tfor feat in feature_doc_count:\n",
        "\t\t\t\tif feature_doc_count[feat] >= MIN_FEATURE_COUNT[self.feature_method.__name__]:\n",
        "\t\t\t\t\tself.feature_vocab[feat] = fid\n",
        "\t\t\t\t\tfid += 1\n",
        "\n",
        "\t\tF = len(self.feature_vocab)\n",
        "\t\tD = len(data)\n",
        "\t\tX = sparse.dok_matrix((D, F))\n",
        "\t\tY = np.zeros(D)\n",
        "\t\tfor idx, (label, feats) in enumerate(data):\n",
        "\t\t\tfor feat in feats:\n",
        "\t\t\t\tif feat in self.feature_vocab:\n",
        "\t\t\t\t\tX[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "\t\t\tY[idx] = 1 if label == \"pos\" else 0\n",
        "\n",
        "\t\treturn X, Y\n",
        "\n",
        "\tdef load_test(self, dataFile):\n",
        "\t\tdata = self.load_data(dataFile)\n",
        "\t\tdata = self.featurize(data)\n",
        "\n",
        "\t\tF = len(self.feature_vocab)\n",
        "\t\tD = len(data)\n",
        "\t\tX = sparse.dok_matrix((D, F))\n",
        "\t\tY = np.zeros(D, dtype = int)\n",
        "\t\tfor idx, (data_id, feats) in enumerate(data):\n",
        "\t\t\t# print (data_id)\n",
        "\t\t\tfor feat in feats:\n",
        "\t\t\t\tif feat in self.feature_vocab:\n",
        "\t\t\t\t\tX[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "\t\t\tY[idx] = data_id\n",
        "\n",
        "\t\treturn X, Y\n",
        "\n",
        "\t# Train model and evaluate on held-out data\n",
        "\tdef evaluate(self, trainX, trainY, devX, devY):\n",
        "\t\t(D,F) = trainX.shape\n",
        "\t\tself.log_reg = linear_model.LogisticRegression(C = L2_REGULARIZATION_STRENGTH[self.feature_method.__name__])\t\n",
        "\t\tself.log_reg.fit(trainX, trainY)\n",
        "\t\ttraining_accuracy = self.log_reg.score(trainX, trainY)\n",
        "\t\tdevelopment_accuracy = self.log_reg.score(devX, devY)\n",
        "\t\tprint(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\t\t\n",
        "\n",
        "\t# Predict labels for new data\n",
        "\tdef predict(self, testX, idsX):\n",
        "\t\tpredX = self.log_reg.predict(testX)\n",
        "\n",
        "\t\tout = open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\")\n",
        "\t\tout.write(\"Id,Expected\\n\")\n",
        "\t\tfor idx, data_id in enumerate(testX):\n",
        "\t\t\tout.write(\"%s,%s\\n\" % (idsX[idx], int(predX[idx])))\n",
        "\t\tout.close()\n",
        "\n",
        "\t# Write learned parameters to file\n",
        "\tdef printWeights(self):\n",
        "\t\tout = open(\"%s_%s\" % (self.feature_method.__name__, \"weights.txt\"), \"w\", encoding=\"utf8\")\n",
        "\t\treverseVocab = [None]*len(self.feature_vocab)\n",
        "\t\tfor feat in self.feature_vocab:\n",
        "\t\t\treverseVocab[self.feature_vocab[feat]] = feat\n",
        "\n",
        "\t\tout.write(\"%.5f\\t__BIAS__\\n\" % self.log_reg.intercept_)\n",
        "\t\tfor (weight, feat) in sorted(zip(self.log_reg.coef_[0], reverseVocab)):\n",
        "\t\t\tout.write(\"%.5f\\t%s\\n\" % (weight, feat))\n",
        "\t\tout.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkcH6MaXEODZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words(text):\n",
        "  word_bag = {}\n",
        "\t# Your code goes here\n",
        "  words = re.split(r\"\\s+\", text)\n",
        "  for word in words:\n",
        "    word = word.strip().lower()\n",
        "    if word in word_bag:\n",
        "      word_bag[word] += 1\n",
        "    else:\n",
        "      word_bag[word] = 1\n",
        "\n",
        "  #for w, c in word_bag:\n",
        "  #  if c > 10:\n",
        "  #    word_bag.pop(w, None)\n",
        "  return word_bag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxKmEqI5JY71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dics = []\n",
        "# Implement your fancy featurization here\n",
        "def fancy_featurize(text):\n",
        "  global dics\n",
        "  features = {}\n",
        "  features.update(lexicon_analysis(text))\n",
        "  features.update(punct_analysis(text))\n",
        "  features.update(rating_analysis(text))\n",
        "  features.update(bag_of_words_test_data(text))\n",
        "  dics.append(features)\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbJCPsr9G-wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert all apostrope negations is, are, would etc into separate words:\n",
        "def clean_text(text):\n",
        "  text = re.sub(r'(is\\s*n\\s*\\'*t)', 'is not ', text)\n",
        "  text = re.sub(r'(I\\s*\\'\\s*m)', 'I am ', text)\n",
        "  text = re.sub(r'you\\s*\\'\\s*re', 'you are ', text)\n",
        "  text =re.sub(r'he\\s*\\'\\s*s', 'he is ', text)\n",
        "  text =re.sub(r'she\\s*\\'\\s*s', 'she is ', text)\n",
        "  text =re.sub(r'we\\s*\\'\\s*re', 'we are ', text)\n",
        "  text =re.sub(r'it\\s*\\'\\s*s', 'it is ', text)\n",
        "  text =re.sub(r'are\\s*n\\s*\\'\\s*t', 'are not ', text)\n",
        "  text =re.sub(r'they\\s*\\'\\s*re', 'they are ', text)\n",
        "  text =re.sub(r'there\\s*\\'\\s*s', 'there is ', text)\n",
        "  text =re.sub(r'was\\s*n\\s*\\'\\s*t', 'was not ', text)\n",
        "  text =re.sub(r'were\\s*n\\s*\\'\\s*t', 'were not ', text)           \n",
        "  text =re.sub(r'I\\s*\\'\\s*ve', 'I have ', text)\n",
        "  text =re.sub(r'you\\s*\\'\\s*ve', 'you have ', text)\n",
        "  text =re.sub(r'we\\s*\\'\\s*ve', 'we have ', text)\n",
        "  text =re.sub(r'they\\s*\\'\\s*ve', 'they have ', text)\n",
        "  text =re.sub(r'has\\s*n\\s*\\'\\s*t', 'has not ', text)\n",
        "  text =re.sub(r'have\\s*n\\s*\\'\\s*t', 'have not ', text) \n",
        "  text =re.sub(r'I\\s*\\'\\s*d', 'I had ', text)\n",
        "  text =re.sub(r'you\\s*\\'\\s*d', 'you had ', text)\n",
        "  text =re.sub(r'he\\s*\\'\\s*d', 'he had ', text)\n",
        "  text =re.sub(r'she\\s*\\'\\s*d', 'she had ', text)\n",
        "  text =re.sub(r'it\\s*\\'\\s*d', 'it had ', text)\n",
        "  text =re.sub(r'we\\s*\\'\\s*d', 'we had ', text)\n",
        "  text =re.sub(r'they\\s*\\'\\s*d', 'they had ', text)\n",
        "  text =re.sub(r'does\\s*n\\s*\\'\\s*t', 'does not ', text) \n",
        "  text =re.sub(r'do\\s*n\\s*\\'\\s*t', 'do not ', text)\n",
        "  text =re.sub(r'did\\s*n\\s*\\'\\s*t', 'did not ', text)\n",
        "  text =re.sub(r'I\\s*\\'\\s*ll', 'I will ', text)\n",
        "  text =re.sub(r'you\\s*\\'\\s*ll', 'you will ', text)\n",
        "  text =re.sub(r'he\\s*\\'\\s*ll', 'he will ', text)\n",
        "  text =re.sub(r'she\\s*\\'\\s*ll', 'she will ', text)\n",
        "  text =re.sub(r'we\\s*\\'\\s*ll', 'we will ', text)\n",
        "  text =re.sub(r'they\\s*\\'\\s*ll', 'they will ', text)\n",
        "  text =re.sub(r'there\\s*\\'\\s*ll', 'there will ', text)\n",
        "  text =re.sub(r'there\\s*\\'\\s*d', 'there had ', text)\n",
        "  text =re.sub(r'ca\\s*n\\s*\\'\\s*t', 'can not ', text)\n",
        "  text =re.sub(r'can\\s*not', 'can not ', text)\n",
        "  text =re.sub(r'could\\s*n\\s*\\'\\s*t', 'could not ', text)\n",
        "  text =re.sub(r'dare\\s*n\\s*\\'\\s*t', 'dare not ', text)\n",
        "  text =re.sub(r'had\\s*n\\s*\\'\\s*t', 'had not ', text)\n",
        "  text =re.sub(r'might\\s*n\\s*\\'\\s*t', 'might not ', text)\n",
        "  text =re.sub(r'must\\s*n\\s*\\'\\s*t', 'must not ', text)\n",
        "  text =re.sub(r'need\\s*n\\s*\\'\\s*t', 'need not ', text)\n",
        "  text =re.sub(r'ought\\s*n\\s*\\'\\s*t', 'ought not ', text)\n",
        "  text =re.sub(r'sha\\s*n\\s*\\'\\s*t', 'shall not ', text)\n",
        "  text =re.sub(r'should\\s*n\\s*\\'\\s*t', 'should not ', text)\n",
        "  text =re.sub(r'used\\s*n\\s*\\'\\s*t', 'used not ', text)\n",
        "  text =re.sub(r'wo\\s*n\\s*\\'\\s*t', 'will not ', text)\n",
        "  text =re.sub(r'would\\s*n\\s*\\'\\s*t', 'would not ', text)\n",
        "  text= re.sub(r'b\\s*\\/\\s*c', 'because ', text)\n",
        "  text =re.sub(r'\\s+\\'\\s*s', ' is ', text)\n",
        "  text =re.sub(r'\\s+u\\s+', ' you ', text)\n",
        "  text =re.sub(r'\\s+dr\\s+', ' doctor ', text)\n",
        "  text = re.sub(r\"(w+o+a*w+|w+o+a+h*)\", 'wow ', text)\n",
        "  text = re.sub(r\"sci[\\s-]+fi\", ' scifi ', text)\n",
        "  text = re.sub(r\"wanna\", ' want to ', text)\n",
        "  text = re.sub(r\"\\s+'\\s*re\", ' are ', text)\n",
        "  text = re.sub(r\"\\s+'\\s*ll\", ' will ', text)\n",
        "  return text\n",
        "                     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trq5SjhF7k_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha09PEyGBmHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stop words set up \n",
        "stop = \"1 2 3 4 5 6 7 8 9 10 11 a re about above after again against all i am an and any are is s as at be because been before being below between both but by can could d\"\n",
        "stop += \"did do does doing down during each few for from further had hadn't has have having he he'd he'll he's her here here's hers herself him himself his how how's i i'd\"\n",
        "stop += \"i'll i'm i've if in into is it it's its itself m me more most my myself of on once only or other ought our ours ourselves out over own same she she'd she'll she's\" \n",
        "stop += \"some that that's the their theirs them themselves then there there's these they they'd they'll they're they've this those through to under until up us ve we we'd we'll\"\n",
        "stop +=  \"we're we've were what what's when when's where where's which while who who's whom why why's with you you'd you'll you're you've your yours yourself yourselves\"\n",
        "ignore = {}\n",
        "for w in stop.split(\" \"):\n",
        "  ignore[w.strip()] = 0\n",
        "#print(ignore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wuJfFjmOO0WD",
        "outputId": "cdbe6d73-28d2-4b0f-eac1-c81f69174692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# subjectivity lexicon set up\n",
        "f = open('/content/drive/My Drive/Colab Notebooks/subj_clues_lib.txt', 'r')\n",
        "lex = {}\n",
        "for line in f:\n",
        "  word = line.split(\" \")\n",
        "  if word[0] == 'type=weaksubj':\n",
        "    strength = 'w'\n",
        "  else: \n",
        "    strength = 's'\n",
        "  \n",
        "  if word[5].strip() == 'priorpolarity=negative':\n",
        "    pos = 0\n",
        "    lex[word[2][6:]] = [strength, pos]\n",
        "  elif word[5].strip() == 'priorpolarity=positive':\n",
        "    pos = 1\n",
        "    lex[word[2][6:].strip()] = [strength, pos]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3btHWHSqMHnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# /10 and /5 ratios mentioned in the reviews\n",
        "def convert(val):\n",
        "  if val == None:\n",
        "    return None\n",
        "  nums = {\"zero\": 0, \"one\": 1, \"two\": 2, \"three\":3, \"four\":4, \"five\":5, \"six\":6, \"seven\":7, \"eight\":8, \"nine\":9, \"ten\":10}\n",
        "  if val in nums:\n",
        "    return nums[val]\n",
        "  elif is_number(val):\n",
        "    return float(val)\n",
        "\n",
        "def rating_analysis(text):\n",
        "  rate = {}\n",
        "  rates = re.findall(r\"(\\d\\.?\\d*\\/10|\\d\\.?\\d*\\/5)\", text)\n",
        "  also = re.findall(r\"\\w+\\s+out\\s+of\\s+10\", text)\n",
        "  last = re.findall(r\"\\w+\\s+out\\s+of\\s+ten\", text)\n",
        "  if len(rates)==0:\n",
        "    return rate\n",
        "  calc = [float(r.split(\"/\")[0])/float(r.split(\"/\")[1]) for r in rates]\n",
        "  calc2 = [convert(str(r.split(\"out of 10\")[0].strip()))/10 for r in also if convert(str(r.split(\"out of 10\")[0].strip()))!=None]\n",
        "  calc3 = [convert(str(r.split(\"out of ten\")[0].strip()))/10 for r in last if convert(str(r.split(\"out of ten\")[0].strip()))!= None]\n",
        "\n",
        "  tot = sum(calc) + sum(calc2) + sum(calc3)\n",
        "  size = len(calc) + len(calc2) + len(calc3)\n",
        "\n",
        "  rate[\"given_rate_avg\"] = (tot/size)*10\n",
        "  return rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDr4xZw1JbuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adds the bag of words representation of the text to feats\n",
        "def bag_of_words_test_data(text):\n",
        "  word_bag = {}\n",
        "  text = clean_text(text.lower())\n",
        "\t# Your code goes here\n",
        "  words = re.split(r\"\\W+\", text)\n",
        "  for word in words:\n",
        "    word = word.strip().lower()\n",
        "    if (word not in ignore) and (not is_number(word)) and (word != \"\"): \n",
        "      if word in word_bag:\n",
        "        word_bag[word] += 1\n",
        "      else:\n",
        "        word_bag[word] = 1\n",
        "\n",
        "  return word_bag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvSplOo3H2aZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#emojis :(  ): (:  :) ;).\n",
        "#punctuation marks number, ?? ?! !! ** @# ... (+) \n",
        "#Examples \"f#@k!ng\" \"w@nk\" \"F * * *in\" \"...\"\n",
        "#hmm \"af\" \"haha\" \"hahha\" \"Ah hah\" \"Ah - ha\"\n",
        "def punct_analysis(text):\n",
        "  info = {}\n",
        "  info[\"all_punc\"] = len(re.findall(r\"[\\!\\\"#\\$%&\\'\\(\\)\\*+,\\-\\.\\/:;<=>?@\\[\\\\\\]\\^_`{|}~]\", text))\n",
        "  info[\"bad_words\"] = len(re.findall(r\"[a-zA-z]+[@#*%\\^\\*!$&\\-]+[a-z]+[\\-%\\^@#\\*!$&a-zA-z]*\", text)) # e.g s@!t, f@#k, f*ck f@#k S@#t w@nk f#@k!ng\n",
        "  info[\"asts_total\"] = len(re.findall(r\"\\**\\s*\\*+[\\s*]*\", text))           # **, *, * * * * usually emphasis or spoiler alerts\n",
        "          \n",
        "  info[\"excl_blocks\"] = len(re.findall(r\"\\!+\\s*\\!+[\\s!1]*\", text))   # e.g !!!!!!!!!, ! ! ! !!, !! ! 11 !\n",
        "  info[\"excl_total\"] = len(re.findall(r\"\\!\",text))                  # TOTAL !s used\n",
        "  info[\"excl_sentences\"] = len(re.findall(r\"\\s*\\!+[\\s!1]*\", text))  # single and block uses of !\n",
        "\n",
        "  info[\"q_blocks\"] = len(re.findall(r\"\\?+\\s*\\?+[\\s?]*\", text))  # e.g ????, ?? ? ?\n",
        "  info[\"q_total\"] = len(re.findall(r\"\\?\",text))                 # TOTAL ? used\n",
        "  info[\"q_sentences\"] = len(re.findall(r\"\\s*\\?+[\\s?]*\", text))  # single and block uses of ?\n",
        "\n",
        "  info[\"mixed_total\"] =len(re.findall(r\"\\?+\\s*\\!+|\\!+\\s*\\?+\",text)) # TOTAL ?! combos used\n",
        "\n",
        "  info[\"long_dots\"] = len(re.findall(r\"\\.+\\s*\\.+[\\s.]*\", text))     # e.g '... ', '.. ', '........... ', '.. . . . . . . .... '\n",
        "  info[\"single_dot_sentences\"] = len(re.findall(r\"(\\w\\.(?!\\s*\\.)|\\w\\s+\\.(?!\\.))\", text)) #only the sentences that end with a single .\n",
        "  info[\"dots_total\"] = len(re.findall(r\"\\.\", text))\n",
        "\n",
        "  info[\"sad_faces\"] = len(re.findall(r\"(\\)+-?:|:-?\\(+|://(?!www)|:/(?!/)|/+:|:'\\(+|\\)+':)\", text)) # TOTAL :( or ): or :/ or ://(not www) or /: or //: or :(( or )): or :'( or  )':\n",
        "  info[\"happy_kiss_wink_face\"] = len(re.findall(r\"(\\*+:|:\\*+|\\(+;|;\\)+|:-?\\)+|\\(+-?:|:-?D+|:d+|:'\\)+|\\(+':)\", text)) \n",
        "  # TOTAL :* or *: ;) (; (: or :) or :D or :d or :)) or ((: or :') or  (':\n",
        "  \n",
        "  info[\"wows_total\"] = len(re.findall(r\"(\\s+w+o+a*w+|\\s+w+o+a*h*)\",text)) #TOTAL wow, woa, woah, woooo*w\n",
        "  info[\"omgs_total\"] = len(re.findall(r\"(\\s+o+m+g+|\\s+o+h+\\s+m+y+\\s+g+o+d+)\",text)) #TOTAL omg, oh my go*d\n",
        "  info[\"laughs_total\"] = len(re.findall(r\"[ah\\s]+\",text)) #TOTAL number of laughter haha hahha\n",
        "  info[\"suprise_thinking\"]= len(re.findall(r\"(\\s+o+h+|\\s+h+mm+)\",text)) # oh and hmm\n",
        "  info[\"wtf_disgust\"] = len(re.findall(r\"w+t+f+\", text)) + len(re.findall(r\"wha+t\\s+the+\\s+f\\s*udge\", text)) + len(re.findall(r\"wha+t\\s+the+\\s+he...\", text)) + \\\n",
        "                        len(re.findall(r\"wha+t\\s+the+\\s+fu?[\\s@#*%\\^\\*!$&]\", text)) # wtf, wtf+, what the fuck, what the hell/heck, what the f[..]k \n",
        "  \n",
        "  info[\"anger_points\"] = info[\"q_blocks\"]*2+ info[\"bad_words\"]*3 + info[\"sad_faces\"] + info[\"wtf_disgust\"]*3 + info[\"long_dots\"]*2 + info[\"mixed_total\"]*2\n",
        "  info[\"happy_points\"] = info[\"wows_total\"]*5 + info[\"laughs_total\"]*5 + info[\"happy_kiss_wink_face\"]*5 + info[\"excl_blocks\"]*10\n",
        "\n",
        "\n",
        "  return info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmSLgRXVwGTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Adds the sentiment lexicon counts of the text to feats\n",
        "# 2. Count capitalized words\n",
        "def lexicon_analysis(text):\n",
        "  lexicons = {}\n",
        "\t# Your code goes here\n",
        "  words = text.split(\" \")\n",
        "  pos_count = 0\n",
        "  pos_strong = 0\n",
        "  pos_uniq = 0\n",
        "  neg_count = 0\n",
        "  neg_strong = 0\n",
        "  neg_uniq = 0\n",
        "  tot = 0\n",
        "  utot = 0\n",
        "  capitalized = 0\n",
        "  \n",
        "  for word in words:\n",
        "    uniq = set()\n",
        "    tot += 1\n",
        "    if word == word.upper():\n",
        "      capitalized += 1\n",
        "    word = word.strip().lower()\n",
        "    if word in lex:\n",
        "      if lex[word][1] == 1: # positive word\n",
        "        pos_count += 1\n",
        "        if lex[word][0] == 's':\n",
        "          pos_strong += 1\n",
        "        if word not in uniq:\n",
        "          utot += 1\n",
        "          pos_uniq += 1\n",
        "          uniq.add(word)\n",
        "      else:                 # negative word\n",
        "        neg_count += 1\n",
        "        if lex[word][0] == 's':\n",
        "          neg_strong += 1\n",
        "        if word not in uniq:\n",
        "          utot += 1\n",
        "          neg_uniq += 1\n",
        "          uniq.add(word)\n",
        "    if word not in uniq:\n",
        "          utot += 1\n",
        "          uniq.add(word)\n",
        "\n",
        "  lexicons[\"pos_sentiment_point\"] = pos_strong*10 + (pos_count-pos_strong)*5\n",
        "  lexicons['pos_strong'] = pos_strong\n",
        "  lexicons['pos_neg_ratio'] = pos_count/neg_count if neg_count != 0 else pos_count\n",
        "  lexicons['pos_neg_diff'] = pos_count-neg_count\n",
        "  lexicons['pos_uniq_tot'] = (pos_uniq/utot)*100\n",
        "  lexicons['pos_count'] = pos_count\n",
        "  \n",
        "  lexicons['caps_total'] = capitalized\n",
        "\n",
        "  lexicons['neg_count'] = neg_count\n",
        "  lexicons['neg_uniq'] = neg_uniq/neg_count if neg_count != 0 else neg_uniq\n",
        "  lexicons['neg_strong'] = neg_strong\n",
        "  lexicons[\"neg_sentiment_point\"] = neg_strong*10 + (neg_count-neg_strong)*5\n",
        "  lexicons[\"points_ratio\"] = (lexicons[\"pos_sentiment_point\"]/lexicons[\"neg_sentiment_point\"])*10 if lexicons[\"neg_sentiment_point\"]!=0 else lexicons[\"pos_sentiment_point\"]\n",
        "\n",
        "  return lexicons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgwgKmYWLlc8",
        "colab_type": "code",
        "outputId": "eb95f21d-7b8d-45fb-9eb5-02a974834a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "# regularization strength to control overfitting (values closer to 0  = stronger regularization)\n",
        "L2_REGULARIZATION_STRENGTH = {\"dumb_featurize\": 1, \"fancy_featurize\": 0.1}\n",
        "\n",
        "# must observe feature at least this many times in training data to include in model\n",
        "MIN_FEATURE_COUNT = {\"dumb_featurize\": 10,  \"fancy_featurize\":10 }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  trainingFile = \"./train.txt\"\n",
        "  evaluationFile = \"./dev.txt\"\n",
        "  testFile = \"./test.txt\"\n",
        "\n",
        "  for feature_method in [dumb_featurize, fancy_featurize]:\n",
        "    sentiment_classifier = SentimentClassifier(feature_method)\n",
        "    trainX, trainY = sentiment_classifier.process(trainingFile, training=True)\n",
        "    devX, devY = sentiment_classifier.process(evaluationFile, training=False)\n",
        "    testX, idsX = sentiment_classifier.load_test(testFile)\n",
        "    sentiment_classifier.evaluate(trainX, trainY, devX, devY)\n",
        "    sentiment_classifier.printWeights()\n",
        "    sentiment_classifier.predict(testX, idsX)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: dumb_featurize, Features: 2, Train accuracy: 0.604, Dev accuracy: 0.611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Method: fancy_featurize, Features: 2007, Train accuracy: 0.824, Dev accuracy: 0.767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjR3bwMfZ6lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as dp"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}